# Spotify MDS Pipeline

## Metodologia e guia

Objetivo: mostrar, passo a passo e sem exigir que saiba programar, como levantar o projeto, ver dados passando pela pipeline e checar resultados nas interfaces (Airflow, MinIO, Metabase).

Metodologia (vis√£o conceitual, 6 passos simples)
1. Simular eventos: um "produtor" gera registros de reprodu√ß√£o (main.py).
2. Enviar para fila/streaming: esses registros v√£o para o Kafka (mensageria).
3. Guardar os dados brutos: um sink salva arquivos no MinIO (S3 compat√≠vel) ‚Äî camada Bronze.
4. Orquestrar e mover: Airflow executa tarefas que l√™em os arquivos e carregam para o armaz√©m (Snowflake).
5. Transformar: dbt aplica transforma√ß√µes (Silver ‚Üí Gold) para criar tabelas prontas para an√°lise.
6. Visualizar: Metabase conecta ao armaz√©m e mostra dashboards com m√©tricas.

O que voc√™ ver√° (sem programar)
- No MinIO: arquivos JSON/Parquet com os eventos gerados.
- No Kafka: t√≥picos recebendo mensagens (pode checar via logs ou console dentro do container).
- No Airflow: um DAG (spotify_pipeline) que executa etapas da pipeline.
- No Snowflake: schemas e tabelas criadas (se usar Snowflake); se n√£o tiver Snowflake, parte das etapas ficar√° parcial.
- No Metabase: dashboards prontos com contagens/aggrega√ß√µes.

Passo a passo pr√°tico (sem saber programar)

Antes: instale Docker + docker-compose e Git. Se n√£o tiver Snowflake, ainda assim voc√™ pode subir tudo localmente ‚Äî s√≥ n√£o ter√° o armaz√©m remoto funcionando.

1) Clonar e copiar vari√°veis
- Abra terminal/PowerShell.
- git clone https://github.com/your/repo.git
- cd repo
- Windows: copy .env.sample .env
- Linux/Mac: cp .env.sample .env
    - Dica: n√£o precisa editar .env para testar a maior parte localmente; s√≥ se for usar Snowflake ajuste SNOWFLAKE_*.

2) Subir infraestrutura (um comando)
- docker-compose up -d --build
    - O Docker baixa imagens e inicia servi√ßos (Kafka, MinIO, Airflow, Metabase). Aguarde ~1‚Äì2 minutos.
    - Se falhar, rode docker-compose logs -f <servi√ßo> para ver erro.

3) Verificar UIs (abra no navegador)
- Airflow: http://localhost:8080 ‚Äî login: airflow / airflow
    - Ative o DAG "spotify_pipeline" e clique em "Trigger" para executar manualmente.
- MinIO Console: http://localhost:9001 ‚Äî usu√°rio: minioadmin / minioadmin
    - Abra o bucket (spotify-raw ou nome do .env) e veja arquivos chegando.
- Metabase: http://localhost:3000 ‚Äî configure senha inicial via UI e veja dashboards prontos.

4) Iniciar o produtor (gera eventos)
- No terminal: python src/main.py --topic spotify.streams --rate 10
    - O script envia eventos para Kafka; cada evento tamb√©m ser√° salvo em MinIO dependendo da configura√ß√£o.
    - Se n√£o souber rodar Python: pode pular este passo e apenas for√ßar o DAG no Airflow para testar ingest√£o/transforma√ß√£o (algumas DAGs podem depender do produtor).

5) Checar resultados
- MinIO: arquivos novos aparecem no bucket raw/bronze.
- Airflow: monitorar execu√ß√µes do DAG; abra logs de cada tarefa para ver progresso.
- Metabase: ap√≥s dbt rodar e dados estarem em Gold, abra dashboards para ver m√©tricas (plays por hora, top tracks, etc).

Comandos r√°pidos √∫teis (copiar/colar)
- docker-compose up -d --build
- docker-compose logs -f kafka
- python src/main.py --topic spotify.streams --rate 10
- docker exec -it <airflow_container> bash   (se precisar inspecionar dentro do container)
- dbt (opcional, para transformar): cd dbt && dbt deps && dbt seed && dbt run --profiles-dir ../dbt --target dev

Dicas pr√°ticas (vis√£o direta)
- Passo inicial: clone o reposit√≥rio, copie .env.sample ‚Üí .env e rode docker-compose up -d --build.
- Para ver o fluxo sem editar c√≥digo: abra as UIs (Airflow, MinIO, Metabase) e acompanhe os logs; a maior parte funciona por configura√ß√£o.
- Teste r√°pido:
    - Suba infra: docker-compose up -d --build
    - Opcional: gerar eventos localmente: python src/main.py --topic spotify.streams --rate 10
    - Ative/execute o DAG spotify_pipeline no Airflow para for√ßar ingest√£o/transforma√ß√£o.
- Principais UIs/URLs:
    - Airflow: http://localhost:8080 (airflow/airflow)
    - MinIO Console: http://localhost:9001 (minioadmin/minioadmin)
    - Metabase: http://localhost:3000
- Onde olhar quando algo falha:
    - Logs dos containers: docker-compose logs -f <servi√ßo>
    - Airflow: logs do scheduler/webserver/task instances
    - MinIO: console e permiss√µes do bucket
    - Kafka: logs do broker e t√≥picos (use kafka-console-consumer dentro do container)
- Corre√ß√µes r√°pidas comuns:
    - Reiniciar e limpar volumes: docker-compose down -v && docker-compose up -d --build
    - DAGs do Airflow n√£o aparecem: verifique montagem da pasta de dags e permiss√µes; reinicie scheduler/webserver
    - Erros de conex√£o com Snowflake: confirme SNOWFLAKE_* no .env e teste com SnowSQL/python
- Comandos √∫teis resumidos:
    - docker-compose up -d --build
    - docker-compose logs -f kafka
    - python src/main.py --topic spotify.streams --rate 10
    - docker exec -it <airflow_container> bash
    - dbt: cd dbt && dbt deps && dbt seed && dbt run --profiles-dir ../dbt --target dev
- Para pedir ajuda: copie o trecho do log com erro e inclua o comando que executou ‚Äî isso acelera o diagn√≥stico.

Observa√ß√£o final: foque em abrir as UIs e checar logs primeiro ‚Äî a maioria dos problemas √© evidenciada ali.
- N√£o precisa editar c√≥digo para ver o fluxo: execute docker-compose e use as UIs.
- Logs e consoles mostram mensagens claras ‚Äî se uma etapa falhar, leia o log e pesquise a mensagem.
- Pe√ßa ajuda copiando o erro exato; isso facilita diagn√≥stico.

Problemas comuns e corre√ß√µes r√°pidas
- Servi√ßo n√£o sobe: docker-compose down -v && docker-compose up -d --build
- Airflow sem DAGs: verifique se a pasta dags est√° montada e permiss√µes (container logs).
- Sem acesso MinIO: verifique credenciais em .env (MINIO_ROOT_USER/PASSWORD).

Resumo final (um par√°grafo)
Este projeto simula todo o fluxo de dados: um produtor gera eventos, Kafka transporta, MinIO guarda o raw, Airflow orquestra movimenta√ß√µes, dbt transforma e Metabase mostra resultados. Para come√ßar sem saber programar, basta clonar, copiar o .env, subir com docker-compose, abrir as UIs (Airflow, MinIO, Metabase) e, opcionalmente, executar python src/main.py para ver os eventos sendo produzidos ‚Äî a interface e os logs guiar√£o o restante do processo.

[![Build Status](https://img.shields.io/badge/build-passing-brightgreen)](https://github.com/your/repo/actions)
[![Docker](https://img.shields.io/badge/docker-available-blue?logo=docker)](https://www.docker.com/)
[![Apache Airflow](https://img.shields.io/badge/Airflow-v2.x-orange?logo=apache-airflow)]
[![Kafka](https://img.shields.io/badge/Kafka-available-red?logo=apachekafka)]
[![Snowflake](https://img.shields.io/badge/Snowflake-ready-9cf?logo=snowflake)]
[![DBT](https://img.shields.io/badge/dbt-ready-ff69b4?logo=dbt)]
[![MinIO](https://img.shields.io/badge/MinIO-ready-ffcc00?logo=minio)]
[![Metabase](https://img.shields.io/badge/Metabase-ready-0052cc?logo=metabase)]
[![License: MIT](https://img.shields.io/badge/license-MIT-green)](./LICENSE)

## üìñ Tabela de Conte√∫dos
- [‚ú® Funcionalidades](#‚ú®-funcionalidades)
- [üèóÔ∏è Arquitetura](#üèóÔ∏è-arquitetura-do-sistema)
- [üöÄ Quick Start (3 minutos)](#üöÄ-quick-start-3-minutos)
- [‚öôÔ∏è Instala√ß√£o Detalhada](#‚öôÔ∏è-instala√ß√£o-detalhada)
- [üîß Configura√ß√£o](#üîß-configura√ß√£o-detalhada)
- [üéØ Como Usar](#üéØ-como-usar)
- [üêõ Troubleshooting](#üêõ-troubleshooting)
- [‚ùì FAQ](#‚ùì-faq)
- [üõ†Ô∏è Estrutura do Projeto](#üõ†Ô∏è-estrutura-do-projeto)
- [üìä Tabelas e Compatibilidade](#üìä-tabelas-e-compatibilidade)
- [üó∫Ô∏è Roadmap](#üó∫Ô∏è-roadmap-e-melhorias-futuras)
- [ü§ù Contribuindo](#ü§ù-contribuindo)

---

‚ú® Funcionalidades
- Simula√ß√£o de dados de streaming do Spotify (produtor: src/main.py)
- Ingest√£o em tempo real com Apache Kafka
- Armazenamento de objetos com MinIO (S3 compat√≠vel)
- Orquestra√ß√£o com Apache Airflow (DAGs para ingest√£o/transform)
- Armazenamento anal√≠tico em Snowflake
- Transforma√ß√µes com dbt (Medallion Architecture: Bronze ‚Üí Silver ‚Üí Gold)
- Visualiza√ß√£o com Metabase (dashboards prontos)
- Containeriza√ß√£o com Docker / docker-compose
- Scripts de verifica√ß√£o e health checks

---

üèóÔ∏è Arquitetura do Sistema
Fluxo de dados (ASCII + emojis)
main.py (produtor) üîÅ ‚Üí Kafka üü® ‚Üí MinIO üóÑÔ∏è ‚Üí Airflow ‚öôÔ∏è ‚Üí Snowflake ‚ùÑÔ∏è ‚Üí dbt üõ†Ô∏è ‚Üí Metabase üìà

Diagrama simples:
```
[ main.py ]  -->  [ Kafka ]  -->  [ MinIO (raw/bronze) ]
                                   |
                                   v
                                [ Airflow ] --> [ Snowflake (silver/gold) ] -- dbt -->
                                                                        |
                                                                        v
                                                                     [Metabase]
```

Explica√ß√£o dos componentes:
- main.py: produtor de eventos que simula plays, usu√°rios, dispositivos, timestamps.
- Kafka: t√≥pico(s) de streaming (e.g., spotify.streams) ‚Üí consumidores simples/Connect.
- MinIO: armazena objetos raw (JSON/Parquet) ‚Äî camada Bronze.
- Airflow: orquestra consumo, landing ‚Üí copy para Snowflake, execu√ß√£o dbt.
- Snowflake: armaz√©m anal√≠tico; tabelas por camada (bronze/silver/gold).
- dbt: transforma e documenta modelos; executa tests.
- Metabase: conex√£o direta ao Snowflake para dashboards.

Padr√£o: Medallion Architecture (Bronze ‚Üí Silver ‚Üí Gold)
- Bronze: dados raw sem transforma√ß√£o (MinIO / Snowflake stage)
- Silver: limpeza, deduplica√ß√£o, enriquecimento
- Gold: agrega√ß√µes e tabelas prontas para BI

---

üöÄ Quick Start (3 minutos)
Pr√©-requisitos: Docker, docker-compose, Git, python >=3.8, Snowflake account (ou usar Snowflake trial).

Comandos EXATOS:

Windows (PowerShell):
```powershell
git clone https://github.com/your/repo.git
cd repo
copy .env.sample .env
docker-compose up -d --build
# aguardar servi√ßos subirem (~30-90s)
docker-compose logs -f kafka   # verificar kafka
# iniciar produtor local (opcional)
python src/main.py
# rodar DAGs/DBT via Airflow webserver/scheduler (veja URLs abaixo)
```

Linux / Mac (bash):
```bash
git clone https://github.com/your/repo.git
cd repo
cp .env.sample .env
docker-compose up -d --build
# verificar servi√ßos
docker-compose logs -f kafka
# opcional: iniciar produtor
python src/main.py
```

Comandos para executar testes:
```bash
pytest --cov=src tests/unit
```

URLs e credenciais padr√£o (vari√°veis em .env; exemplo):
- Airflow UI: http://localhost:8080 (user: airflow / pass: airflow)
- MinIO Console: http://localhost:9001 (user: minioadmin / pass: minioadmin)
- Kafka (broker): localhost:9092
- Metabase: http://localhost:3000 (setup inicial via UI)
- Snowflake: usar credenciais em .env (SNOWFLAKE_ACCOUNT, USER, PASSWORD, ROLE, WAREHOUSE, DATABASE, SCHEMA)

Observa√ß√£o: ajuste credenciais em .env antes de executar.

---

‚öôÔ∏è Instala√ß√£o Detalhada

1. Clonar reposit√≥rio
2. Copiar vari√°veis de exemplo
   - Linux/Mac: cp .env.sample .env
   - Windows: copy .env.sample .env
3. Editar .env com credenciais do Snowflake e op√ß√µes do MinIO
4. Levantar infra com Docker Compose:
   - docker-compose up -d --build
5. Executar health checks:
   - Linux/Mac: ./scripts/check_env.sh
   - Python: python scripts/health_check.py

Instru√ß√µes espec√≠ficas por SO:
- Windows: use PowerShell com permiss√£o de administrador; assegure WSL2 se usar Docker Desktop.
- Linux: garanta permiss√µes de rede e ulimits para Kafka.
- Mac: habilite recursos do Docker Desktop (CPU/RAM adequados).

Arquivo .env (vari√°veis obrigat√≥rias)
- MINIO_ROOT_USER=
- MINIO_ROOT_PASSWORD=
- MINIO_BUCKET=
- KAFKA_BROKER=localhost:9092
- SNOWFLAKE_ACCOUNT=
- SNOWFLAKE_USER=
- SNOWFLAKE_PASSWORD=
- SNOWFLAKE_ROLE=
- SNOWFLAKE_WAREHOUSE=
- SNOWFLAKE_DATABASE=
- SNOWFLAKE_SCHEMA=
- DBT_PROFILES_DIR=./dbt
- AIRFLOW__CORE__FERNET_KEY=...
- AIRFLOW__CORE__LOAD_EXAMPLES=False

Configura√ß√£o de conex√µes Airflow (exemplos via Airflow UI/Admin -> Connections)
- kafka_conn (kafka://localhost:9092)
- minio_s3 (s3://minioadmin:minioadmin@minio:9000)
- snowflake_default (using Snowflake hook; account, user, password)

Snowflake ‚Äî passos r√°pidos
1. Conectar via SnowSQL ou UI.
2. Criar warehouse/database/schema:
```sql
CREATE WAREHOUSE IF NOT EXISTS MDS_WH WITH WAREHOUSE_SIZE = 'XSMALL' WAREHOUSE_TYPE = 'STANDARD' AUTO_SUSPEND = 60;
CREATE DATABASE IF NOT EXISTS SPOTIFY_MDS;
USE DATABASE SPOTIFY_MDS;
CREATE SCHEMA IF NOT EXISTS RAW;
CREATE SCHEMA IF NOT EXISTS SILVER;
CREATE SCHEMA IF NOT EXISTS GOLD;
```
3. Criar role/user e conceder privil√©gios (exemplo minimal):
```sql
CREATE ROLE IF NOT EXISTS mds_role;
GRANT USAGE ON WAREHOUSE MDS_WH TO ROLE mds_role;
GRANT USAGE ON DATABASE SPOTIFY_MDS TO ROLE mds_role;
GRANT ALL ON SCHEMA SPOTIFY_MDS.RAW TO ROLE mds_role;
GRANT ALL ON SCHEMA SPOTIFY_MDS.SILVER TO ROLE mds_role;
GRANT ALL ON SCHEMA SPOTIFY_MDS.GOLD TO ROLE mds_role;
```
4. Preencha vari√°veis SNOWFLAKE_* no .env.

---

üéØ Como Usar
Executar produtor:
```bash
python src/main.py --topic spotify.streams --rate 10
```
Verificar t√≥picos Kafka:
```bash
docker exec -it kafka-container kafka-topics --bootstrap-server localhost:9092 --list
docker exec -it kafka-container kafka-console-consumer --bootstrap-server localhost:9092 --topic spotify.streams --from-beginning --max-messages 5
```
Airflow:
- Acesse http://localhost:8080
- Ative o DAG spotify_pipeline
- For√ßar execu√ß√£o para testes

dbt:
```bash
cd dbt
dbt deps
dbt seed
dbt run --profiles-dir ../dbt --target dev
dbt test
```

Exemplo de registro gerado (JSON):
```json
{
  "user_id":"u_123",
  "track_id":"t_456",
  "played_at":"2025-11-06T12:34:56Z",
  "device":"mobile",
  "duration_ms":210000
}
```

---

üêõ Troubleshooting (erros comuns)

1. Kafka n√£o sobe / broker n√£o dispon√≠vel
   - Comando: docker-compose logs -f kafka
   - Solu√ß√£o: aumentar ulimits, remover volumes e reiniciar: docker-compose down -v && docker-compose up -d

2. Airflow DAGs n√£o aparecem
   - Verifique AIRFLOW__CORE__DAGS_FOLDER, permiss√µes e reinicie scheduler/webserver
   - Logs: docker-compose logs -f airflow-scheduler

3. Conex√£o Snowflake falha
   - Teste com SnowSQL ou python:
```python
from snowflake.connector import connect
conn = connect(user='USER', password='PW', account='ACCT')
```
   - Confira SNOWFLAKE_ACCOUNT e ROLE

4. MinIO acesso negado
   - Console: http://localhost:9001
   - Credenciais default: minioadmin:minioadmin (alterar em .env)

Comandos √∫teis para diagn√≥stico:
- docker ps
- docker-compose logs -f <service>
- kafka-topics, kafka-console-consumer (dentro do container)
- dbt debug --profiles-dir dbt

Logs a checar:
- Airflow: scheduler, webserver, worker
- Kafka: broker, zookeeper
- MinIO: server
- Snowflake: ver query history no UI

---

‚ùì FAQ
Q: Posso usar Snowflake trial?
A: Sim. Preencha SNOWFLAKE_ACCOUNT e credenciais no .env.

Q: Quanto tempo demora para subir tudo?
A: ~30s‚Äì2min dependendo da m√°quina; Kafka/MinIO/DB inicializa√ß√£o podem levar mais.

Q: Preciso de Internet para rodar?
A: Sim, para baixar imagens Docker na primeira vez e para Snowflake se estiver usando conta remota.

Q: Posso substituir MinIO por S3 real?
A: Sim ‚Äî configure endpoint e credenciais S3 no .env e ajuste conex√µes.

---

üìä Tabelas e Compatibilidade

Tabela de compatibilidade de SO
| Sistema | Docker | Testado | Observa√ß√µes |
|---|---:|---:|---|
| Windows 10/11 (WSL2 recomendado) | ‚úÖ | ‚úÖ | Use PowerShell/WSL2 |
| Ubuntu 20.04+ | ‚úÖ | ‚úÖ | Ajuste ulimits para Kafka |
| macOS (Intel/Apple Silicon) | ‚úÖ | ‚úÖ | Docker Desktop recomendado |

Tabela de componentes principais
| Componente | Fun√ß√£o | Local |
|---|---|---|
| Kafka | Ingest√£o streaming | container kafka |
| MinIO | Armazenamento objetos (Bronze) | container minio |
| Airflow | Orquestra√ß√£o | container airflow |
| Snowflake | Armazenamento anal√≠tico | cloud |
| dbt | Transforma√ß√µes (Silver/Gold) | dbt/ |
| Metabase | Visualiza√ß√µes | container metabase |
| Producer (main.py) | Simula eventos | src/main.py |

---

üó∫Ô∏è Roadmap e melhorias futuras
- Autentica√ß√£o centralizada e secrets manager (Vault)
- Kafka Connect para CDC e sinks adicionais
- Deploy Kubernetes (Helm charts)
- CI/CD para dbt models e tests (GitHub Actions)
- Monitoramento (Prometheus + Grafana)
- Suporte a particionamento e compacta√ß√£o no S3/MinIO

---

üì∏ Sugest√µes de capturas de tela (placeholders)
- <!-- SCREENSHOT: Airflow UI mostrando DAG ativo ‚Äî capture http://localhost:8080 with DAG spotify_pipeline expanded -->
- <!-- SCREENSHOT: MinIO Console mostrando bucket spotify-raw -->
- <!-- SCREENSHOT: Metabase dashboard com m√©tricas de plays por hora -->
- <!-- SCREENSHOT: dbt docs site / lineage gr√°fico -->

---

üõ†Ô∏è Estrutura do Projeto
- src/: produtor e micro-servi√ßos de ingest√£o
- config/: arquivos de configura√ß√£o
- db/: scripts SQL e migrations
- dbt/: modelos dbt, seeds e profiles
- dags/: DAGs do Airflow
- tests/: testes unit√°rios e de integra√ß√£o
- scripts/: health checks, helpers
- docker/: Dockerfiles e overrides
- .github/workflows/: CI/CD

---

‚úÖ Valida√ß√£o e health checks inclu√≠dos
- scripts/check_env.sh (bash)
- scripts/health_check.py (python)
- dbt tests para modelos cr√≠ticos
- DAGs Airflow com sensors e retries

---

ü§ù Contribuindo
- Leia CONTRIBUTING.md
- Use branches feature/* e PRs
- Escreva testes para mudan√ßas e atualize docs/dbt docs

---

Licen√ßa
MIT ‚Äî veja LICENSE para detalhes.


Este projeto simula um ambiente de produ√ß√£o real. Pipeline completo de dados em tempo real, pronto para uso com configura√ß√£o m√≠nima ‚Äî ideal para aprendizado de Modern Data Stack.

## Como rodar
1. Copiar .env.sample para .env
2. Rodar `docker-compose up -d`
3. Executar `python src/main.py`
4. Rodar testes com `pytest --cov=src tests/unit`
